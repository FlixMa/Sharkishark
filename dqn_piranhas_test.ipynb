{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import gym\n",
    "import gym_piranhas\n",
    "\n",
    "env = gym.make('piranhas-v0')\n",
    "# TODO seed here?\n",
    "nb_actions = 3 #env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution3D, Flatten, Dense, Activation, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "INPUT_SHAPE = 10\n",
    "DIMENSIONS  = 3\n",
    "WINDOW_LENGTH = 4  # The number of past states we consider\n",
    "input_shape = (WINDOW_LENGTH, INPUT_SHAPE, INPUT_SHAPE, DIMENSIONS)\n",
    "output_length = 2 + 1  # two coordinates + 1 direction\n",
    "# TODO test with 100 or more outputs (two-hot encoding)\n",
    "\n",
    "model = Sequential()\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # tensorflow ordering: (batch, width, height, channels)\n",
    "    model.add(Permute((1, 2, 3, 4), input_shape=input_shape))\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # theano ordering: (batch, channels, width, height)\n",
    "    model.add(Permute((1, 4, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering')\n",
    "    \n",
    "# The actual network structure\n",
    "# results in a (6 x 6 x 32) output volume\n",
    "model.add(Convolution3D(32, (1, 5, 5), activation='relu', data_format='channels_last'))\n",
    "# results in a (4 x 4 x 64) output volume\n",
    "model.add(Convolution3D(64, (1, 3, 3), activation='relu', data_format='channels_last'))\n",
    "# results in a (2 x 2 x 64) output volume\n",
    "model.add(Convolution3D(64, (1, 3, 3), activation='relu', data_format='channels_last'))\n",
    "# flattens the result (vector of size 256)\n",
    "model.add(Flatten())\n",
    "# add fully-connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# map to output: coordinates and move\n",
    "model.add(Dense(output_length, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "from core.state import GameState\n",
    "from core.util import FieldState, PlayerColor\n",
    "\n",
    "\n",
    "class PiranhasProcessor(Processor):\n",
    "    \n",
    "    def __init__(self, game_state, game_settings):\n",
    "        assert isinstance(game_state, GameState)\n",
    "        assert isinstance(game_settings, GameSettings)\n",
    "        self.game_state = game_state\n",
    "        self.game_settings = game_settings\n",
    "    \n",
    "    # Preprocess gameState to an image the neural net receives\n",
    "    # the image is of three dimensions, representing\n",
    "    # us, the opponent and obstructions on the 10x10 board\n",
    "    # as a 10x10 channel each. The view is normalized so that\n",
    "    # our fishes are always on the left and right side.\n",
    "    def process_observation(self):\n",
    "        # observation is of type gamestate\n",
    "        observation = {}\n",
    "        observation[\"board\"] = np.zeros((10, 10, 3))  ## (us, opponent, kraken)\n",
    "        observation[\"board\"][:][:][0] = np.where(game_state.board == FieldState.fromPlayerColor(game_settings.ourColor))\n",
    "        observation[\"board\"][:][:][1] = np.where(game_state.board == FieldState.fromPlayerColor(game_settings.ourColor.otherColor))\n",
    "        observation[\"board\"][:][:][2] = np.where(game_state.board == FieldState.OBSTRUCTED)\n",
    "        \n",
    "        if game_settings.ourColor != game_settings.startPlayerColor:\n",
    "            # we normalize the board so that we are always the starting player\n",
    "            # who has fishes on the left and right hand side\n",
    "            observation[\"board\"] = np.rot90(observation[\"board\"])\n",
    "        \n",
    "        ## TODO? opponent[\"nr_fish\"] and opponent[\"biggest_group\"]\n",
    "        observation[\"turn_nr\"]        = game_tate.turn\n",
    "        observation[\"begins\"]         = game_settings.startPlayerColor\n",
    "        \n",
    "        return observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "# copied from https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_atari.py\n",
    "# Add memory\n",
    "\n",
    "# https://stackoverflow.com/questions/47140642/what-does-the-episodeparametermemory-of-keras-rl-do\n",
    "# for explanation of parameters\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "processor = PiranhasProcessor()\n",
    "\n",
    "# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n",
    "# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n",
    "# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n",
    "# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n",
    "# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory,\n",
    "               processor=processor, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1750000 steps ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c070dad87297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# log to file every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFileLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1750000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# After training is done, we save the final weights one more time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pwb/lib/python3.5/site-packages/rl/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2ab6b8b334a2>\u001b[0m in \u001b[0;36mprocess_observation\u001b[0;34m(self, game_state_and_settings)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_state_and_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# observation is of type gamestate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mgame_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state_and_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mgame_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_state_and_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGameState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "mode = 'train'\n",
    "env.dqn = dqn\n",
    "if mode == 'train':\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that now you can use the built-in Keras callbacks!\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env.name)\n",
    "    checkpoint_weights_filename = 'dqn_' + env.name + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env.name)\n",
    "    # Add a checkpoint every 250000 iterations\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "    # log to file every 100 iterations\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "elif mode == 'test':\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env.name)\n",
    "    #if args.weights:\n",
    "    #    weights_filename = args.weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
