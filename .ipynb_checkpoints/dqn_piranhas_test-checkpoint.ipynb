{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import gym\n",
    "import gym_piranhas\n",
    "\n",
    "env = gym.make('piranhas-v0')\n",
    "# TODO seed here?\n",
    "nb_actions = len(env.action_space.spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 10, 10, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 32)          2432      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 91,139\n",
      "Trainable params: 91,139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Convolution3D, Flatten, Dense, Activation, Permute\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "INPUT_SHAPE = 10\n",
    "DIMENSIONS  = 3\n",
    "# WINDOW_LENGTH = 4  # The number of past states we consider\n",
    "input_shape = (INPUT_SHAPE, INPUT_SHAPE, DIMENSIONS) # (WINDOW_LENGTH, INPUT_SHAPE, INPUT_SHAPE, DIMENSIONS)\n",
    "# output_length = env.action_space.n  # two coordinates + 1 direction\n",
    "# TODO test with 100 or more outputs (two-hot encoding)\n",
    "\n",
    "model = Sequential()\n",
    "if K.image_dim_ordering() == 'tf':\n",
    "    # tensorflow ordering: (batch, width, height, channels)\n",
    "    # model.add(Permute((1, 2, 3, 4), input_shape=input_shape))\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "elif K.image_dim_ordering() == 'th':\n",
    "    # theano ordering: (batch, channels, width, height)\n",
    "    # model.add(Permute((1, 4, 2, 3), input_shape=input_shape))\n",
    "    model.add(Permute((3, 1, 2), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering')\n",
    "    \n",
    "# The actual network structure\n",
    "# results in a (6 x 6 x 32) output volume\n",
    "model.add(Convolution2D(32, (5, 5), activation='relu', data_format='channels_last'))\n",
    "# results in a (4 x 4 x 64) output volume\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu', data_format='channels_last'))\n",
    "# results in a (2 x 2 x 64) output volume\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu', data_format='channels_last'))\n",
    "# flattens the result (vector of size 256)\n",
    "model.add(Flatten())\n",
    "# add fully-connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# map to output: coordinates and move\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "from core.state import GameState\n",
    "from core.util import FieldState, PlayerColor\n",
    "\n",
    "\n",
    "class PiranhasProcessor(Processor):\n",
    "    \n",
    "    def __init__(self, game_state, game_settings):\n",
    "        assert isinstance(game_state, GameState)\n",
    "        assert isinstance(game_settings, GameSettings)\n",
    "        self.game_state = game_state\n",
    "        self.game_settings = game_settings\n",
    "    \n",
    "    # Preprocess gameState to an image the neural net receives\n",
    "    # the image is of three dimensions, representing\n",
    "    # us, the opponent and obstructions on the 10x10 board\n",
    "    # as a 10x10 channel each. The view is normalized so that\n",
    "    # our fishes are always on the left and right side.\n",
    "    def process_observation(self):\n",
    "        # observation is of type gamestate\n",
    "        observation = {}\n",
    "        observation[\"board\"] = np.zeros((10, 10, 3))  ## (us, opponent, kraken)\n",
    "        observation[\"board\"][:][:][0] = np.where(game_state.board == FieldState.fromPlayerColor(game_settings.ourColor))\n",
    "        observation[\"board\"][:][:][1] = np.where(game_state.board == FieldState.fromPlayerColor(game_settings.ourColor.otherColor))\n",
    "        observation[\"board\"][:][:][2] = np.where(game_state.board == FieldState.OBSTRUCTED)\n",
    "        \n",
    "        if game_settings.ourColor != game_settings.startPlayerColor:\n",
    "            # we normalize the board so that we are always the starting player\n",
    "            # who has fishes on the left and right hand side\n",
    "            observation[\"board\"] = np.rot90(observation[\"board\"])\n",
    "        \n",
    "        ## TODO? opponent[\"nr_fish\"] and opponent[\"biggest_group\"]\n",
    "        observation[\"turn_nr\"]        = game_tate.turn\n",
    "        observation[\"begins\"]         = game_settings.startPlayerColor\n",
    "        \n",
    "        return observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)\n",
    "\n",
    "# copied from https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_atari.py\n",
    "# Add memory\n",
    "\n",
    "# https://stackoverflow.com/questions/47140642/what-does-the-episodeparametermemory-of-keras-rl-do\n",
    "# for explanation of parameters\n",
    "memory = SequentialMemory(limit=1000000, window_length=1) # WINDOW_LENGTH\n",
    "# processor = PiranhasProcessor()\n",
    "# preprocessing is done in onGameStateUpdate in the environment\n",
    "\n",
    "# Select a policy. We use eps-greedy action selection, which means that a random action is selected\n",
    "# with probability eps. We anneal eps from 1.0 to 0.1 over the course of 1M steps. This is done so that\n",
    "# the agent initially explores the environment (high eps) and then gradually sticks to what it knows\n",
    "# (low eps). We also set a dedicated eps value that is used during testing. Note that we set it to 0.05\n",
    "# so that the agent still performs some random actions. This ensures that the agent cannot get stuck.\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy, memory=memory, # processor=processor,\n",
    "               nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'try:\\n    game_client.wait_until_stopped()\\nexcept:\\n    game_client.stop()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start server here\n",
    "\n",
    "import core\n",
    "# start the client-server communication\n",
    "host = '127.0.0.1'\n",
    "port = 13050\n",
    "\n",
    "\n",
    "game_client = core.communication.GameClient(host, port, env)\n",
    "\n",
    "game_client.start() # connect to the server\n",
    "game_client.join() # join a game\n",
    "\n",
    "\"\"\"try:\n",
    "    game_client.wait_until_stopped()\n",
    "except:\n",
    "    game_client.stop()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1750000 steps ...\n",
      "last data: '<protocol>\\n  <sc.protocol.responses.CloseConnection/>\\n</protocol>'\n",
      "exception in receive:  OSError('no data received',)\n",
      "data on exception: ''\n",
      "remote peer closed the socket: no data received\n"
     ]
    }
   ],
   "source": [
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "\n",
    "mode = 'train'\n",
    "# env.dqn = dqn\n",
    "if mode == 'train':\n",
    "    # Okay, now it's time to learn something! We capture the interrupt exception so that training\n",
    "    # can be prematurely aborted. Notice that now you can use the built-in Keras callbacks!\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env.name)\n",
    "    checkpoint_weights_filename = 'dqn_' + env.name + '_weights_{step}.h5f'\n",
    "    log_filename = 'dqn_{}_log.json'.format(env.name)\n",
    "    # Add a checkpoint every 250000 iterations\n",
    "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "    # log to file every 100 iterations\n",
    "    callbacks += [FileLogger(log_filename, interval=100)]\n",
    "    dqn.fit(env, callbacks=callbacks, nb_steps=1750000, log_interval=10000)\n",
    "\n",
    "    # After training is done, we save the final weights one more time.\n",
    "    dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 10 episodes.\n",
    "    dqn.test(env, nb_episodes=10, visualize=False)\n",
    "elif mode == 'test':\n",
    "    weights_filename = 'dqn_{}_weights.h5f'.format(env.name)\n",
    "    #if args.weights:\n",
    "    #    weights_filename = args.weights\n",
    "    dqn.load_weights(weights_filename)\n",
    "    dqn.test(env, nb_episodes=10, visualize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
